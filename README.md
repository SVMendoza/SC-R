SC-R: a supervised classification methodology

Maps of land use, carbon levels, forest degradation, or pests damage are crucial for climate change adaptation and restoration strategies for territorial planning. These maps derived from earth observation data help both local farmers and governments make informed management decisions. However, they are often built using unbalanced data and/or pseudo-replicates (signatures), which can create problems related to the poor performance of supervised classification algorithms. 


![myimage-alt-tag](../master/SC-R/Process.png)
(https://github.com/SVMendoza/SC-R/Process.png)


SC-R is presented, a processing system designed to address supervised image classification problems from satellites or drones using machine learning techniques. SC-R integrates a set of strategies aimed at maximizing the performance of learning algorithms, considering the commonly used correlated data for training and computational limitations, with the goal of generating reliable results.

SC-R allows the user to choose between training a single machine learning model or using multiple models, as well as deciding whether to combine the predictions obtained through an ensemble result. This ensemble can be performed using a multinomial approach or using the arithmetic mode.

The models available in SC-R include Random Forest, Support Vector Machine (SVM) with polynomial kernel, SVM with sigmoid kernel, XGBoost, and a neural network.

The pipeline offers an image preprocessing tool using convolution (a typical Deep Learning process), using filters of 16, 32, 64, or 124 channels. If you choose convolution, it is recommended to apply a subsequent dimensionality reduction using Principal Component Analysis (PCA). This technique seeks to simplify the feature space, eliminate noise, and improve the computational efficiency of learning algorithms.

If you decide to use a convolutional neural network (CNN), dimensionality reduction may not be advisable due to the high computational cost involved in using a CNN. In this case, PCA techniques could be applied to reduce the number of bands before image convolution. PCA could also be applied in the first stage of convolution if the user is training a deep network.

SC-R is structured in modules with a functional approach, which facilitates its development and allows for constant updating and improvement. At the moment, SC-R only supports data in Shapefile format with polygon geometry projected in the same way as the image to be classified.

This system is simple but efficient, allowing good results to be obtained even with small datasets and good computational efficiency. Model ensembling allows for the generation of a final consensus map that incorporates the best features of individual models and provides performance metrics based on a confusion matrix.
Models are trained sequentially, ensuring that at each step, representative observations from each polygon are used, which helps overcome the lack of data independence, a common challenge when training models in these contexts. This approach allows capturing the maximum variability in the data. Additionally, data is randomly selected to train the models in n iterations, choosing the models with the best performance on the validation data. In the case of Random Forest, the top 10 validated models are selected and assembled into a single final Random Forest.

How does it work?

**Usage**

classiFunction(name.shape, file.img, name.CLASES, OPEN, SAVE, n.core, propVal, nsize=NULL, sel.n, ndt, dt.balance,=FALSE, Normalize=TRUE, selModel=c(), epochs=NULL, batch=NULL, gpu=NULL)


**Arguments**

name.shape 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name of the shapefile with the **.shp** extension. 

file.img 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name of the image to be classified with the .tif extension. 

OPEN 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; working directory where name.shape and file.img are stored. 

name.CLASES 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name of the response/classification variable that identifies the class in the shapefile attribute table. 

SAVE 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; directory where all processing generated by the system will be saved. 

n.core	 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; number of cores (processors) to use, by default NULL (uses all available cores). 

propVal 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; by default, is NULL, defines the number of observations to use for model validation. 

Nsize 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; number of models to fit; in the case of XGBoost, only a single model is fitted. 

ndt 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; only for Random Forest. By default, is 100, and represents the number of trees per model to build. 

sel.n 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; number of observations per polygon to be selected. **: Directory where the R scripts are located.

dt.balance 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is a Boolean value, default is FALSE. If TRUE, the algorithm forces the data subsets to be balanced by category. 

Normalize 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is a Boolean value, default is TRUE, which allows the image to be normalized to a scale between 0 and 1.

selModel 		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; selects the model(s) that should be trained and used for classification. 

epochs 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is NULL by default, and is only for adjusting a neural network. 

batch 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is NULL by default, and is only for adjusting a neural network.

gpu 			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is NULL by default. If you are running a neural network and have an NDVIA GPU, you can set it to TRUE. Otherwise, the network will be trained using the CPU.

```plaintext
# NOT RUN


```

